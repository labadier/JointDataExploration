{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from Concept_Mining.models.ConceptMining import ConceptMiner\n",
    "from Concept_Mining.utils.settings import settings\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 70\n",
    "images = [i for i in glob(\"../dataset/Images/*.jpg\")]\n",
    "Miner = ConceptMiner(images = images, load_model= True)\n",
    "\n",
    "# encodings, captions = Miner.get_image_caption(get_encodings = True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('encodings.pkl', 'wb') as f:\n",
    "#     pickle.dump({'encodings':encodings, 'captions': captions}, f)\n",
    "with open('encodings.pkl', 'rb') as f:\n",
    "\tdata = pickle.load(f)\n",
    "\tencodings = data['encodings']\n",
    "\tcaptions = data['captions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_model, generated_concepts = Miner.concept_modeling(encodings = encodings, \n",
    "                                                           captions = captions, \n",
    "                                                           batch_size = batch_size)\n",
    "\n",
    "with open(\"generated_concepts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(generated_concepts, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluser topics by semantic similarity \n",
    "- Cluser concepts\n",
    "- Sample concepts taking as mean the centroid +- a random noice to simulate user externalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from umap import UMAP\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "with open(\"generated_concepts.pkl\", \"rb\") as f:\n",
    "    generated_concepts = list(set(pickle.load(f)) - {\"\"})\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_embedding(sentences, model, tokenizer, batch_size = 16):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        output = None\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            encoded_input = tokenizer(sentences[i: i + batch_size], padding=True, return_tensors='pt')\n",
    "\n",
    "            model_output = model(**encoded_input)\n",
    "            sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "            output = sentence_embeddings if output is None else torch.cat([output, sentence_embeddings])\n",
    "\n",
    "    return output\n",
    "\n",
    "concepts_embeddings = get_embedding(generated_concepts, model, tokenizer, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "similarity_matrix = cosine_similarity(concepts_embeddings)\n",
    "\n",
    "# Apply Spectral Clustering\n",
    "num_clusters = 20\n",
    "spectral = SpectralClustering(n_clusters=num_clusters, affinity='precomputed', random_state=42)\n",
    "labels = spectral.fit_predict(similarity_matrix)\n",
    "\n",
    "# Group words by topic\n",
    "topic_groups = {}\n",
    "for word, cluster in zip(generated_concepts, labels):\n",
    "    topic_groups.setdefault(cluster, []).append(word)\n",
    "    \n",
    "with open(\"generated_concepts.pkl\", \"wb\") as f:\n",
    "\tpickle.dump({'concepts':generated_concepts,\n",
    "              'topic_groups': topic_groups}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data for simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import random\n",
    "\n",
    "with open(\"generated_concepts.pkl\", \"rb\") as f:\n",
    "\tdata = pickle.load(f)\n",
    "\tgenerated_concepts = data['concepts']\n",
    "\ttopic_groups = data['topic_groups']\n",
    "\n",
    "with open('encodings.pkl', 'rb') as f:\n",
    "\tdata = pickle.load(f)\n",
    "\tencodings = data['encodings']\n",
    "\tcaptions = data['captions']\n",
    "\n",
    "images = [i for i in glob(\"../dataset/Images/*.jpg\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encode all text and images to speed up training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "batch_size = 70\n",
    "# encode all images using output of a single text\n",
    "image_encodings = []\n",
    "\n",
    "for batch in tqdm(range(0, len(images), batch_size)):\n",
    "\timage = [Image.open(i) for i in images[batch: batch + batch_size]]\n",
    "\tinputs = processor(text=[\"a photo of a cat\"] * len(image), images=image, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\toutputs = model(**inputs, return_dict = True)\n",
    "\timage_encodings += [outputs.image_embeds.cpu().detach()]\n",
    "\n",
    "image_encodings = torch.cat(image_encodings)\n",
    "\n",
    "# encode all concepts using output of a single image\n",
    "concept_encodings = []\n",
    "for batch in tqdm(range(0, len(generated_concepts), batch_size)):\n",
    "\tconcepts = generated_concepts[batch: batch + batch_size]\n",
    "\tinputs = processor(text=concepts, images=[Image.open(images[0])], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\toutputs = model(**inputs, return_dict = True)\n",
    "\tconcept_encodings += [outputs.text_embeds.cpu().detach()]\n",
    "\n",
    "concept_encodings = torch.cat(concept_encodings)\n",
    "with open(\"clip_encodings.pkl\", \"wb\") as f:\n",
    "\tpickle.dump({'image_encodings': image_encodings,\n",
    "\t\t\t  'concept_encodings': concept_encodings}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_instance = {i:[] for i in generated_concepts}\n",
    "\n",
    "for i, path in enumerate(images):\n",
    "\tfor c in captions[i].split():\n",
    "\t\tif c in generated_concepts:\n",
    "\t\t\tconcept_instance[c] += [path]\n",
    "\n",
    "remove_topics = []\n",
    "for i in topic_groups:\n",
    "\tremove = [j for j in topic_groups[i] if len(concept_instance[j]) < 10]\n",
    "\tfor j in remove:\n",
    "\t\ttopic_groups[i].remove(j)\n",
    "\tprint(f\"Topic {i}:\", len(topic_groups[i]))\n",
    "\tif len(topic_groups[i]) < 8:\n",
    "\t\tremove_topics.append(i)\n",
    "\n",
    "for i in remove_topics:\n",
    "\tdel topic_groups[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\t\n",
    "plt.bar(range(len(generated_concepts)), sorted([len(i) for i in concept_instance.values()]), log = True)\n",
    "plt.title(\"Concepts Frequency\")\n",
    "plt.xlabel(\"Concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(topic_groups)), [len(topic_groups[i]) for i in topic_groups], color = 'r')\n",
    "plt.title(\"Topics Size\")\n",
    "plt.xlabel(\"Topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# from Actor_Critic import AdaptationEngine\n",
    "from Concept_Mining.models.ContextualBandit import AdaptationEngine\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import random; random.seed(42)\n",
    "\n",
    "\n",
    "def encode_actions(action, n_actions):\n",
    "\n",
    "\tif not isinstance(action, torch.Tensor):\n",
    "\t\taction = torch.tensor(action)\n",
    "\treturn torch.nn.functional.one_hot(action, num_classes=n_actions).float()\n",
    "\n",
    "def preprocess_state(state):\n",
    "\treturn torch.tensor(state).float().reshape(1, -1)\n",
    "\n",
    "def get_entropy(preferences):\n",
    "\treturn -torch.sum(preferences * torch.log(preferences), dim = -1)\n",
    "\n",
    "episodes = 0\n",
    "episode_length = 1000\n",
    "dataset_coverage_step = .75\n",
    "gamma = 0.99\n",
    "buffer = []\n",
    "actions_embedd_dim = 2\n",
    "buffer_size = 245\n",
    "\n",
    "lr_actor = 0.01\n",
    "lr_critic = 0.0001\n",
    "\n",
    "temperature = .2\n",
    "final_temperature = 0.1\n",
    "decay_rate = 0.999  # Decay per episode\n",
    "\n",
    "\n",
    "# env = gym.make('MountainCar-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Pi = Actor(state_dim = env.observation_space.shape[0], action_dim = actions_embedd_dim, lr_optimizer = lr_actor)\n",
    "# V = Critic(state_dim = env.observation_space.shape[0], lr_optimizer = lr_critic)\n",
    "Agent = AdaptationEngine(state_dim = env.observation_space.shape[0],\n",
    "\t\t\t\t\t\t action_space_len=env.action_space.n,\n",
    "\t\t\t\t\t\t \taction_dim=env.action_space.n,\n",
    "\t\t\t\t\t\t\tlr_actor = lr_actor,\n",
    "\t\t\t\t\t\t\tlr_critic = lr_critic,\n",
    "\t\t\t\t\t\t\tgamma = gamma,\n",
    "\t\t\t\t\t\t\tbuffer_size = buffer_size,\n",
    "\t\t\t\t\t\t\tsample_temperature = temperature,\n",
    "\t\t\t\t\t\t\tfinal_temperature = final_temperature,\n",
    "\t\t\t\t\t\t\ttemperatura_decay = decay_rate)\n",
    "\n",
    "actions_encode = encode_actions([i  for i in range(env.action_space.n)], env.action_space.n).to(Agent.device)\n",
    "\n",
    "episode_history = {'rewards': [],\n",
    "\t\t    'loss_critic': [], \n",
    "\t\t\t'loss_actor': []}\n",
    "\n",
    "history = {'rewards': [],\n",
    "\t\t    'loss_critic': [], \n",
    "\t\t\t'loss_actor': []}\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "\tprev_state, _ = env.reset()\n",
    "\tprev_state = preprocess_state(prev_state)\n",
    "\n",
    "\tepisode_history = {'rewards': [],\n",
    "\t\t'loss_critic': [], \n",
    "\t\t'loss_actor': [],\n",
    "\t\t'entropy': []}\n",
    "\t\t\t\n",
    "\titera = tqdm(range(episode_length))\n",
    "\titera.set_description(f\"Episode {episode}\")\n",
    "\n",
    "\tfor step in itera:\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tAgent.eval()\n",
    "\t\t\tpreferences = Agent.policy(prev_state, actions_encode)\t\n",
    "\t\t\t# print(preferences)\n",
    "\t\t\t\n",
    "\t\t\tif random.random() < Agent.temperature:\n",
    "\t\t\t\taction = torch.randint(0, env.action_space.n, (1,)).item()\n",
    "\t\t\telse:\n",
    "\t\t\t\taction = torch.argmax(preferences).item()\n",
    "\t\t\t\n",
    "\t\t\tstate, reward, done, truncated, _ = env.step(action)\n",
    "\t\t\tstate = preprocess_state(state)\n",
    "\t\tAgent.train()\n",
    "\n",
    "\t\tAgent.push_buffer(state=prev_state,\n",
    "\t\t\t\t\t action=torch.tensor(action), \n",
    "\t\t\t\t\t reward=reward, \n",
    "\t\t\t\t\t next_state=state, \n",
    "\t\t\t\t\t done=done)\n",
    "\t\t\n",
    "\t\tepisode_history['rewards'].append(reward)\n",
    "\t\t# episode_history['entropy'].append(get_entropy(preferences).mean().item())\n",
    "\n",
    "\t\tif Agent.buffer_size <= len(Agent.buffer):\n",
    "\t\t\tactor_loss = Agent.optimization_step(actions_encode)\n",
    "\t\t\t# actor_loss, critic_loss = Agent.optimization_step(actions_encode)\n",
    "\t\t\t# episode_history['loss_critic'].append(critic_loss)\n",
    "\t\t\tepisode_history['loss_actor'].append(actor_loss)\n",
    "\n",
    "\t\tprev_state = state\n",
    "\n",
    "\t\titera.set_postfix({'Temperature': f'{temperature:.4f}',\n",
    "\t\t\t\t\t\t'Reward': sum(episode_history['rewards']),\n",
    "\t\t\t\t\t\t# 'entropy': sum(episode_history['entropy']),\n",
    "\t\t\t\t\t\t# 'Loss Critic': sum(episode_history['loss_critic']),\n",
    "\t\t\t\t\t\t'Loss Actor': sum(episode_history['loss_actor'])} )\n",
    "\t\tif done or truncated:\n",
    "\t\t\thistory['rewards'].append(sum(episode_history['rewards']))\n",
    "\t\t\t# history['loss_critic'].append(sum(episode_history['loss_critic']))\n",
    "\t\t\thistory['loss_actor'].append(sum(episode_history['loss_actor']))\n",
    "\t\t\tbreak\n",
    "\n",
    "\ttemperature = max(final_temperature, temperature * decay_rate)\n",
    "\tif Agent.max_reward is None or Agent.max_reward < sum(episode_history['rewards']):\n",
    "\t\tAgent.max_reward = sum(episode_history['rewards'])\n",
    "\t\tAgent.Actor.save(f\"bandit.pt\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent.eval()\n",
    "print(Agent.policy(prev_state, actions_encode))\n",
    "prev_state\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['rewards'])\n",
    "plt.title(\"Rewards\")\n",
    "plt.xlabel(\"Iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# from Actor_Critic import AdaptationEngine\n",
    "from Concept_Mining.models.ContextualBandit import AdaptationEngine\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "def show_state(env, step=0):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render())\n",
    "    plt.title(f\"Step: {step}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "Agent = AdaptationEngine(state_dim = env.observation_space.shape[0],\n",
    "\t\t\t\t\t\t action_space_len=env.action_space.n,\n",
    "\t\t\t\t\t\t\taction_dim=env.action_space.n)\n",
    "\n",
    "# Agent = AdaptationEngine(state_dim = env.observation_space.shape[0],\n",
    "\t\t\t\t\t\t \n",
    "# \t\t\t\t\t\t \taction_dim=env.action_space.n,\n",
    "# \t\t\t\t\t\t\tlr_actor = lr_actor,\n",
    "# \t\t\t\t\t\t\tlr_critic = lr_critic,\n",
    "# \t\t\t\t\t\t\tgamma = gamma,\n",
    "# \t\t\t\t\t\t\tbuffer_size = buffer_size,\n",
    "# \t\t\t\t\t\t\tsample_temperature = temperature,\n",
    "# \t\t\t\t\t\t\tfinal_temperature = final_temperature,\n",
    "# \t\t\t\t\t\t\ttemperatura_decay = decay_rate)\n",
    "                            \n",
    "Agent.Actor.load('bandit.pt')\n",
    "Agent.eval()\n",
    "# env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "\n",
    "prev_state, info = env.reset()\n",
    "prev_state = preprocess_state(prev_state)\n",
    "print(prev_state)\n",
    "for step in range(int(100)):\n",
    "\tpreferences =  Agent.policy(prev_state, actions_encode)\n",
    "\n",
    "\taction = preferences.argmax()\n",
    "\n",
    "\tobservation, reward, terminated, _, _ = env.step(action.item())\n",
    "\tprint(action)\n",
    "\n",
    "\tprev_state = preprocess_state(observation)\n",
    "\tprev_action = action\n",
    "\n",
    "\tshow_state(env, step=step)\n",
    "\n",
    "\tif terminated:\n",
    "\t\tprint(observation)\n",
    "\t\tbreak\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna for settings (hyperparameters - actor critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random; random.seed(0)\n",
    "import numpy as np; np.random.seed(0)\n",
    "\n",
    "from utils import train_agent\n",
    "\n",
    "settings = {'episode_length': 1000,\n",
    "            'episodes': 3000,\n",
    "            'gamma': 0.99,\n",
    "            'decay_rate': 0.95,\n",
    "            'buffer_size': 256,\n",
    "            'lr_critic': 5e-4, #3e-4 #0.002159000868210698,\n",
    "            'lr_actor': 3e-4, #1e-4#0.0007674507120792033,\n",
    "            'temperature': 5,\n",
    "\t\t\t'final_temperature': 1}\n",
    "    \n",
    "\n",
    "average_reward, deviation_reward, history = train_agent(settings, use_mlflow = False, save_suffix = \"_best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simualte Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "from Concept_Mining.models.SimulationEnvironment import explore_environment\n",
    "from Concept_Mining.models.Actor_Critic import AdaptationEngine\n",
    "from Concept_Mining.utils.settings import  pretraining_settings as settings\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_entropy(preferences):\n",
    "\treturn -torch.sum(preferences * torch.log(preferences), dim = -1)\n",
    "\n",
    "with open(\"clip_encodings.pkl\", \"rb\") as f:\n",
    "\tdata = pickle.load(f)\n",
    "\timage_encodings = data['image_encodings']\n",
    "\tactions_encode = data['concept_encodings']\n",
    "\n",
    "with open(\"generated_concepts.pkl\", \"rb\") as f:\n",
    "\tdata = pickle.load(f)\n",
    "\tgenerated_concepts = data['concepts']\n",
    "\ttopic_groups = data['topic_groups']\n",
    "\n",
    "with open('encodings.pkl', 'rb') as f:\n",
    "\tcaptions = pickle.load(f)['captions']\n",
    "\n",
    "images = [i for i in glob(\"../dataset/Images/*.jpg\")]\n",
    "\n",
    "Agent = AdaptationEngine(state_dim = image_encodings.shape[1] + actions_encode.shape[1],\n",
    "\t\t\t\t\t\t \taction_dim=actions_encode.shape[1],\n",
    "\t\t\t\t\t\t\taction_space_len=actions_encode.shape[0],\n",
    "\t\t\t\t\t\t\tlr_actor = settings.lr_actor,\n",
    "\t\t\t\t\t\t\tlr_critic = settings.lr_critic,\n",
    "\t\t\t\t\t\t\tgamma = settings.gamma,\n",
    "\t\t\t\t\t\t\tbuffer_size = settings.buffer_size,\n",
    "\t\t\t\t\t\t\tsample_temperature = settings.temperature,\n",
    "\t\t\t\t\t\t\tfinal_temperature = settings.final_temperature,\n",
    "\t\t\t\t\t\t\ttemperature_decay = settings.decay_rate)\n",
    "\n",
    "def get_trajectory( env_snapshot, state, trajectory_len: int = 10 ):\n",
    "\t\n",
    "\tcurrent_path = []\n",
    "\tprocessed_state = env_snapshot.preprocess_state(state)\n",
    "\n",
    "\tfor _ in range(trajectory_len):\n",
    "\t\t\n",
    "\t\tpreferences = Agent.policy(processed_state, actions_encode)\n",
    "\t\taction = preferences.argmax() #torch.multinomial(preferences, 1).squeeze(-1)\n",
    "\t\t\n",
    "\t\tcurrent_path.append(action.item())\n",
    "\t\tprocessed_state = env_snapshot.preprocess_state(state, current_path=current_path)\n",
    "\t\n",
    "\treturn current_path\n",
    "\n",
    "env = explore_environment(topic_groups=topic_groups, \n",
    "\t\t\t\t\t\t  concepts=generated_concepts,\n",
    "\t\t\t\t\t\t  concepts_encode=actions_encode,\n",
    "\t\t\t\t\t\t  images=images,\n",
    "\t\t\t\t\t\t  images_caption=captions,\n",
    "\t\t\t\t\t\t  images_encode=image_encodings, \n",
    "\t\t\t\t\t\t  threshold=10)\n",
    "\n",
    "\n",
    "Agent.Actor.load('actor_9.pt')\n",
    "Agent.eval()\n",
    "# Agent.Critic.load('critic_best.pt')\n",
    "prediction = []\n",
    "remaining = []\n",
    "\n",
    "taken_actions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor episode in range(1):\n",
    "\n",
    "\t\tepisode_index, (prev_state, feedback, _) = env.reset()\n",
    "\t\tprev_state = env.preprocess_state(prev_state)\n",
    "\n",
    "\t\tenv.externalized.add(feedback)\n",
    "\t\t\t\t\n",
    "\t\titera = tqdm(range(len(env.topic)))\n",
    "\t\titera.set_description(f\"Episode {episode}\")\n",
    "\n",
    "\t\tfor step in itera:\n",
    "\n",
    "\t\t\t# actions_trajectory = get_trajectory(env, prev_state, trajectory_len = 10)\n",
    "\n",
    "\t\t\t# prediction.append(actions_trajectory.copy())\n",
    "\t\t\t# remaining.append(env.topic[env.step_index:].copy())\n",
    "\t\t\tpreferences = Agent.policy(prev_state, actions_encode)\n",
    "\t\t\taction = torch.multinomial(preferences, 1).squeeze(-1)\n",
    "\n",
    "\t\t\tstate, feedback, _, done = env.step( action.item() )\n",
    "\t\t\ttaken_actions.append(action.item())\n",
    "\t\t\tprint(action.item(), generated_concepts[action.item()])\n",
    "\t\t\tstate = env.preprocess_state(state)\n",
    "\t\t\t# state, feedback, _, done = env.step( actions_trajectory[0] )\n",
    "\t\t\tenv.externalized.add(feedback)\n",
    "\t\t\t# actions_trajectory = state\n",
    "\t\t\tprev_state = state\n",
    "\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "np.random.seed(42)\n",
    "embeddings_1 = actions_encode[env.topic]\n",
    "\n",
    "embeddings_2 = actions_encode[taken_actions] \n",
    "Set1 = [\"Externalized\"]*len(embeddings_1) \n",
    "Set2 = [\"Proposed\"]*len(embeddings_2)  # Labels for second set\n",
    "\n",
    "labels_1 = [generated_concepts[i] for i in env.topic] \n",
    "labels_2 = [generated_concepts[i] for i in taken_actions] # Labels for second set\n",
    "\n",
    "# Concatenate embeddings\n",
    "embeddings = np.vstack([embeddings_1, embeddings_2])\n",
    "labels = labels_1 + labels_2  # Merge labels\n",
    "Sets = Set1 + Set2\n",
    "\n",
    "# Dimensionality Reduction (Use TSNE or UMAP)\n",
    "reducer = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\n",
    "df[\"Set\"] = Sets  # Assign label to distinguish sets\n",
    "df[\"Label\"] = labels  # Assign label for hover\n",
    "\n",
    "\n",
    "# Custom color mapping (Red for Set 1, Blue for Set 2)\n",
    "color_map = {\"Set 1\": \"red\", \"Set 2\": \"blue\"}\n",
    "\n",
    "# Interactive Plot\n",
    "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"Set\", hover_data=[\"Label\"], title=\"Comparison of Two Embedding Sets\",\n",
    "                 color_discrete_map=color_map, size_max=30)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "from Concept_Mining.models.SimulationEnvironment import explore_environment\n",
    "\n",
    "\n",
    "with open(\"clip_encodings.pkl\", \"rb\") as f:\n",
    "\tdata = pickle.load(f)\n",
    "\timage_encodings = data['image_encodings']\n",
    "\tactions_encode = data['concept_encodings']\n",
    "\n",
    "with open(\"generated_concepts.pkl\", \"rb\") as f:\n",
    "\tdata = pickle.load(f)\n",
    "\tgenerated_concepts = data['concepts']\n",
    "\ttopic_groups = data['topic_groups']\n",
    "\n",
    "with open('encodings.pkl', 'rb') as f:\n",
    "\tcaptions = pickle.load(f)['captions']\n",
    "\n",
    "images = [i for i in glob(\"../dataset/Images/*.jpg\")]\n",
    "\n",
    "env = explore_environment(topic_groups=topic_groups, \n",
    "\t\t\t\t\t\tconcepts=generated_concepts,\n",
    "\t\t\t\t\t\tconcepts_encode=actions_encode,\n",
    "\t\t\t\t\t\timages=images,\n",
    "\t\t\t\t\t\timages_caption=captions,\n",
    "\t\t\t\t\t\timages_encode=image_encodings, \n",
    "\t\t\t\t\t\tthreshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.reset(1)\n",
    "# episode_index, (prev_state, feedback, _) = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "12\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "12\n",
      "14\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "12\n",
      "14\n",
      "7\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "12\n",
      "14\n",
      "7\n",
      "4\n",
      "12\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "12\n",
      "14\n",
      "7\n",
      "4\n",
      "12\n",
      "13\n",
      "5\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "12\n",
      "14\n",
      "7\n",
      "4\n",
      "12\n",
      "13\n",
      "5\n",
      "16\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "12\n",
      "14\n",
      "7\n",
      "4\n",
      "12\n",
      "13\n",
      "5\n",
      "16\n",
      "0\n",
      "5\n",
      "7\n",
      "7\n",
      "4\n",
      "13\n",
      "11\n",
      "4\n",
      "4\n",
      "0\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "12\n",
      "14\n",
      "7\n",
      "4\n",
      "12\n",
      "13\n",
      "5\n",
      "16\n",
      "0\n",
      "5\n",
      "7\n",
      "7\n",
      "4\n",
      "13\n",
      "11\n",
      "4\n",
      "4\n",
      "0\n",
      "9\n",
      "13\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "12\n",
      "14\n",
      "7\n",
      "4\n",
      "12\n",
      "13\n",
      "5\n",
      "16\n",
      "0\n",
      "5\n",
      "7\n",
      "7\n",
      "4\n",
      "13\n",
      "11\n",
      "4\n",
      "4\n",
      "0\n",
      "9\n",
      "13\n",
      "1\n",
      "4\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "\u001b[92mAvg. Hit Rate: 0.0631 \u001b[0m\n",
      "\u001b[92mAvg. Reward: -59.1667 \u001b[0m\n",
      "\u001b[92mAvg. KL: 1666.3770 \u001b[0m\n",
      "\u001b[92mAvg. Matching: 0.1340 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from models.SimulationEnvironment import explore_environment\n",
    "from utils.utils_bandits import plot_exploration\n",
    "from utils.settings import  pretraining_settings as settings\n",
    "\n",
    "from utils.metrics import get_matchings, kl_metric\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "with open(\"clip_encodings.pkl\", \"rb\") as f:\n",
    "\tdata = pickle.load(f)\n",
    "\timage_encodings = data['image_encodings']\n",
    "\tactions_encode = data['concept_encodings']\n",
    "\n",
    "with open(\"generated_concepts.pkl\", \"rb\") as f:\n",
    "\tdata = pickle.load(f)\n",
    "\tgenerated_concepts = data['concepts']\n",
    "\ttopic_groups = data['topic_groups']\n",
    "\n",
    "with open('encodings.pkl', 'rb') as f:\n",
    "\tcaptions = pickle.load(f)['captions']\n",
    "\n",
    "images = [i for i in glob(\"../dataset/Images/*.jpg\")]\n",
    "\n",
    "env_tmp = explore_environment(topic_groups=topic_groups, \n",
    "\t\t\t\t\t\tconcepts=generated_concepts,\n",
    "\t\t\t\t\t\tconcepts_encode=actions_encode,\n",
    "\t\t\t\t\t\timages=images,\n",
    "\t\t\t\t\t\timages_caption=captions,\n",
    "\t\t\t\t\t\timages_encode=image_encodings, \n",
    "\t\t\t\t\t\tthreshold=10)\n",
    "\n",
    "save_plot = True\n",
    "\n",
    "def select_greedy_action(externalized, images, action_encode, taken_actions):\n",
    "\t\n",
    "\tcontext = action_encode[list(externalized | set(taken_actions))]\n",
    "\timages = images[:, :-action_encode.shape[1]]\n",
    "\n",
    "\n",
    "\tcompatibility_text =  cosine_similarity(action_encode, context).mean(axis = -1)\n",
    "\t#mask compatibility of index already taken\n",
    "\n",
    "\tcompatibility_image = cosine_similarity(action_encode, images).mean(axis = -1)\n",
    "\tcompatibility = compatibility_text*0.5 + compatibility_image*0.5\n",
    "\t\n",
    "\tcompatibility[list(externalized | set(taken_actions))] = -100\n",
    "\n",
    "\t#sample action using compatibiliy values as distribution\n",
    "\taction = torch.argmax(torch.tensor(compatibility)).item()\n",
    "\treturn action\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "\n",
    "\tepisode_history = {'rewards': [],\n",
    "\t\t\t\t'loss_actor': [],\n",
    "\t\t\t\t'hit_rate': [],}\n",
    "\n",
    "\thistory = {'rewards': [],\n",
    "\t\t\t'episodes_rewards': {},\n",
    "\t\t\t\t'loss_actor': [],\n",
    "\t\t\t\t'hit_rate': {},\n",
    "\t\t\t\t'matching': [],\n",
    "\t\t\t\t'kl_divergency': []}\n",
    "\n",
    "\tfor episode in range(120):\n",
    "\n",
    "\t\tepisode_index, (prev_state, feedback, _) = env_tmp.reset()\n",
    "\t\tprint(episode_index)\n",
    "\t\tprev_state, prev_action_seq = env_tmp.preprocess_state(prev_state)\n",
    "\n",
    "\t\tepisode_history = {'rewards': [],\n",
    "\t\t\t'hit_rate': []}\n",
    "\t\tif episode_index not in history['episodes_rewards']:\n",
    "\t\t\thistory['episodes_rewards'][episode_index] = []\n",
    "\t\t\thistory['hit_rate'][episode_index] = []\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tenv_tmp.externalized.add(feedback)\n",
    "\t\t\n",
    "\t\tif not save_plot:\n",
    "\t\t\titera = tqdm(range(len(env_tmp.topic)))\n",
    "\t\t\titera.set_description(f\"Episode {episode}\")\n",
    "\t\telse:\n",
    "\t\t\titera = range(len(env_tmp.topic))\n",
    "\t\t\n",
    "\t\ttaken_actions = []\n",
    "\t\tfor step in itera:\n",
    "\n",
    "\t\t\taction = select_greedy_action(env_tmp.externalized, prev_state, actions_encode, taken_actions)\n",
    "\n",
    "\t\t\tstate, feedback, reward, done, is_hit = env_tmp.step( action )\n",
    "\t\t\ttaken_actions.append(action)\n",
    "\t\t\t# print(action.item(), generated_concepts[action.item()])\n",
    "\t\t\tstate, action_seq = env_tmp.preprocess_state(state)\n",
    "\t\t\t# state, feedback, _, done = env.step( actions_trajectory[0] )\n",
    "\t\t\tenv_tmp.externalized.add(feedback)\n",
    "\t\t\t# actions_trajectory = state\n",
    "\t\t\tprev_state = state\n",
    "\t\t\tprev_action_seq = action_seq\n",
    "\n",
    "\t\t\tepisode_history['rewards'].append(reward)\n",
    "\t\t\t# episode_history['entropy'].append(get_entropy(preferences).mean().item())\n",
    "\t\t\tepisode_history['hit_rate'].append(is_hit)\n",
    "\n",
    "\n",
    "\t\t\tif done:\n",
    "\t\t\t\thistory['rewards'].append(sum(episode_history['rewards']))\n",
    "\t\t\t\tbreak\t\t\n",
    "\t\thistory['episodes_rewards'][episode_index].append(sum(episode_history['rewards']).item())\n",
    "\t\thistory['hit_rate'][episode_index].append(sum(episode_history['hit_rate'])/len(episode_history['hit_rate']))\n",
    "\t\t\n",
    "\t\ty = [generated_concepts[i] for i in env_tmp.externalized]\n",
    "\t\ty_hat = [generated_concepts[i] for i in taken_actions]\n",
    "\n",
    "\t\thistory['matching'].append(get_matchings(y, y_hat))\n",
    "\t\thistory['kl_divergency'].append(kl_metric(y=y, y_hat=y_hat, reference=generated_concepts))\n",
    "\n",
    "\t\tplot_exploration(actions_encode = actions_encode, env = env_tmp, \n",
    "\t\t\t\t\ttaken_actions = taken_actions, generated_concepts = generated_concepts, \n",
    "\t\t\t\t\tsave_plot = save_plot, episode_index = episode_index,\n",
    "\t\t\t\t\toutput_path = 'outputs/greedy')\n",
    "\t\t\n",
    "\t\t# break\n",
    "\n",
    "from utils.utils_bandits import bcolors\n",
    "print(f\"{bcolors.OKGREEN}Avg. Hit Rate: {np.mean([np.mean(history['hit_rate'][i]) for i in history['hit_rate'].keys()]):.4f} {bcolors.ENDC}\")\n",
    "print(f\"{bcolors.OKGREEN}Avg. Reward: {np.mean([np.mean(history['episodes_rewards'][i]) for i in history['episodes_rewards'].keys()]):.4f} {bcolors.ENDC}\")\n",
    "print(f\"{bcolors.OKGREEN}Avg. KL: {np.mean(history['kl_divergency']):.4f} {bcolors.ENDC}\")\n",
    "print(f\"{bcolors.OKGREEN}Avg. Matching: {np.mean(history['matching']):.4f} {bcolors.ENDC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [generated_concepts[i] for i in env_tmp.externalized]\n",
    "y_hat = [generated_concepts[i] for i in taken_actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /app/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab808d482c104d7baaa6a958879f5c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2a4f02db164b54bc1263aaf88f64b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765735abce06402b9da083b0bbece825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79bca6335ee44c8b6db3b84ec4cc473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0b3d445f434230b5237e4bb9aea1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf46b7cd9c943e7ad58931dc5f164d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/jde/Concept_Mining/utils/metrics.py:51: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3697.)\n",
      "  (mu_2 - mu_1).T @ sigma_2_inv @ (mu_2 - mu_1) -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m1 = get_matchings(y, y_hat)\n",
    "m2 = kl_metric(y, y_hat, generated_concepts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.016666666666666666, 508.1134254363879)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

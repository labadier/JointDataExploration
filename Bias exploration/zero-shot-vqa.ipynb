{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b030789fdcc484080260ada807f361d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load model: 5.531465530395508\n"
     ]
    }
   ],
   "source": [
    "# test.py\n",
    "import torch, os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5-int4', trust_remote_code=True, \n",
    "                                  torch_dtype=torch.float16)\n",
    "print('Time to load model:', time.time()-start)\n",
    "@torch.no_grad()\t\n",
    "def get_logits(self,\n",
    "\t\t\t\t\tmsgs: list[dict],\n",
    "\t\t\t\t\timages: list[list[torch.Tensor]],\n",
    "\t\t\t\t\ttokenizer: AutoTokenizer,\n",
    "\t\t\t\t\ttgt_sizes: list[int],\n",
    "\t\t\t\t\t) -> torch.Tensor:\n",
    "\t\n",
    "\tinput_id_list=[tokenizer.apply_chat_template([msg], tokenize=True, add_generation_prompt=False) for msg in msgs]\n",
    "\timg_list=images\n",
    "\n",
    "\tbs = len(input_id_list)\n",
    "\tif img_list == None:\n",
    "\t\timg_list = [[] for i in range(bs)]\n",
    "\tassert bs == len(img_list)\n",
    "\n",
    "\tmodel_inputs = model._process_list(tokenizer, input_id_list, max_inp_length = 2024)\n",
    "\n",
    "\n",
    "\tpixel_values = []\n",
    "\tfor i in range(bs):\n",
    "\t\timg_inps = []\n",
    "\t\tfor img in img_list[i]:\n",
    "\t\t\timg_inps.append(img.to(model.device))\n",
    "\t\tif img_inps:\n",
    "\t\t\tpixel_values.append(img_inps)\n",
    "\t\telse:\n",
    "\t\t\tpixel_values.append([])\n",
    "\tmodel_inputs[\"pixel_values\"] = pixel_values\n",
    "\tmodel_inputs['tgt_sizes'] = [torch.vstack(tgt_sizes)]\n",
    "\n",
    "\twith torch.inference_mode():\n",
    "\t\t(\n",
    "\t\t\tmodel_inputs[\"inputs_embeds\"],\n",
    "\t\t\tvision_hidden_states,\n",
    "\t\t) = model.get_vllm_embedding(model_inputs)\n",
    "\n",
    "\t\treturn model.llm(inputs_embeds = model_inputs[\"inputs_embeds\"])\n",
    "\n",
    "model.get_logits = get_logits\n",
    "model.yes_tokens = [9642, 9891, 14331, 20137, 41898, 58841, 60844, 77830, 85502]\n",
    "model.no_tokens = [2201, 2822, 6673, 9173, 9278, 17184, 18847, 34200, 38089, 39522]\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../Concept_Mining/generated_concepts.pkl\", \"rb\") as f:\n",
    "\tdata = pickle.load(f)\n",
    "\tgenerated_concepts = data['concepts']\n",
    "\ttopic_groups = data['topic_groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import zero_shot_MINICPM_QA\n",
    "from glob import glob\n",
    "\n",
    "images = glob(f\"../dataset/Images/*.jpg\")\n",
    "\n",
    "prediction, _ = zero_shot_MINICPM_QA(model, images, generated_concepts, \n",
    "\t\t\t\t\t\t\t\ttokenizer, batch_size = 70,\n",
    "\t\t\t\t\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.469440650939942 0.10949265816237837\n"
     ]
    }
   ],
   "source": [
    "z = [6.557735919952393, 6.600693225860596, 6.412484169006348,\n",
    " 6.292077541351318, 6.484212398529053]\n",
    "import numpy as np\n",
    "print(np.mean(z), np.std(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from utils import zero_shot_MINICPM_QA\n",
    "import numpy as np\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "concepts = [\n",
    " \"picture of roads made out of soil\",\n",
    " \"picture of people\",\n",
    " \"picture of roads made out of asphalt\",\n",
    " \"picture of people wearing dark clothes\",\n",
    " \"picture of people wearing colorful clothes\"]\n",
    "\n",
    "images = glob(f\"s1/*.png\")\n",
    "zero_labels = [json.loads(open(f\"{images[i]}.json\").read())['metadata']['labels'] for i in range(len(images))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:50<00:00,  1.11s/it]\n",
      "100%|██████████| 100/100 [01:47<00:00,  1.07s/it]\n",
      "100%|██████████| 100/100 [01:48<00:00,  1.08s/it]\n",
      "100%|██████████| 100/100 [01:58<00:00,  1.18s/it]\n",
      "100%|██████████| 100/100 [01:55<00:00,  1.15s/it]\n",
      "100%|██████████| 100/100 [02:00<00:00,  1.20s/it]\n",
      "100%|██████████| 100/100 [01:56<00:00,  1.16s/it]\n",
      "100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "100%|██████████| 100/100 [02:01<00:00,  1.21s/it]\n",
      "100%|██████████| 100/100 [01:55<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "times = []\n",
    "\n",
    "for i in range(10):\n",
    "\tstart = time.time()\n",
    "\tprediction, _ = zero_shot_MINICPM_QA(model, images, concepts, \n",
    "\t\t\t\t\t\t\t\ttokenizer, batch_size = 16,\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\ttimes.append(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.89601318836212 7.869818195886419\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(times), np.std(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric per class: [0.76470588 0.99453552 0.24489796 0.89655172 0.74074074]\n",
      "Mean metric: 0.7282863651081939\n"
     ]
    }
   ],
   "source": [
    "\n",
    "one_hot = np.zeros((len(images), len(concepts)))\n",
    "for j, i in enumerate(zero_labels):\n",
    "    one_hot[j, i] = 1\n",
    "    \n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(one_hot, prediction.cpu().numpy(), average=None)\n",
    "print(\"Metric per class:\", f1)\n",
    "print(\"Mean metric:\", np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
